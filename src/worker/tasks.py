# src/worker/tasks.py - FINAL, CLEANED VERSION

import os
import time
import redis

# Imports for the workflow execution task
import io
import geopandas as gpd
from minio import Minio
from celery.utils.log import get_task_logger

# Import the Celery app instance from your celery_app.py
from .celery_app import celery

# Import the real executor from your engine
from src.core.executors.workflow_executor import WorkflowExecutor

# Redis client for storing job status updates
try:
    redis_client = redis.Redis(
        host=os.getenv("REDIS_HOST", "localhost"), 
        port=6379, 
        db=0, 
        decode_responses=True
    )
except Exception as e:
    print(f"Warning: Redis connection failed in tasks: {e}")
    redis_client = None

# Initialize logger
logger = get_task_logger(__name__)

# This is now the primary task for running the heavy GIS work.
@celery.task(name="engine.execute_workflow", bind=True)
def execute_geospatial_workflow(self, plan: dict):
    """
    Receives a pre-generated GIS plan, executes it using the real WorkflowExecutor,
    and saves the final GeoDataFrame result to a MinIO bucket.
    
    Args:
        plan (dict): The workflow plan dictionary generated by WorkflowGenerator
        
    Returns:
        dict: Contains bucket and object_name for the saved result
    """
    job_id = self.request.id
    logger.info(f"[Job: {job_id}] Starting workflow execution.")
    
    try:
        # Initialize MinIO client inside the task for process safety
        minio_client = Minio(
            os.getenv("MINIO_ENDPOINT", "minio:9000"),
            access_key=os.getenv("MINIO_ROOT_USER"),
            secret_key=os.getenv("MINIO_ROOT_PASSWORD"),
            secure=False
        )
        
        # Update task state for Celery's built-in status tracking
        self.update_state(
            state='PROGRESS',
            meta={'stage': 'initializing', 'progress': 0, 'job_id': job_id}
        )
        
        # Update Redis with initial status
        if redis_client:
            redis_client.hset(
                f"job:{job_id}",
                mapping={
                    "status": "PROGRESS",
                    "stage": "executing_workflow",
                    "progress": "0"
                }
            )
        
        # Instantiate and run the real executor with the provided plan
        logger.info(f"[Job: {job_id}] Initializing WorkflowExecutor.")
        
        self.update_state(
            state='PROGRESS',
            meta={'stage': 'workflow_execution', 'progress': 25, 'job_id': job_id}
        )
        
        executor = WorkflowExecutor()
        
        logger.info(f"[Job: {job_id}] Executing workflow plan with {len(plan.get('steps', []))} steps.")
        
        self.update_state(
            state='PROGRESS',
            meta={'stage': 'processing_steps', 'progress': 50, 'job_id': job_id}
        )
        
        result_gdf: gpd.GeoDataFrame = executor.execute(plan)
        
        self.update_state(
            state='PROGRESS',
            meta={'stage': 'preparing_output', 'progress': 75, 'job_id': job_id}
        )
        
        # Convert result to GeoJSON bytes for upload
        logger.info(f"[Job: {job_id}] Converting {len(result_gdf)} features to GeoJSON.")
        result_geojson_str = result_gdf.to_json()
        result_geojson_bytes = result_geojson_str.encode('utf-8')
        
        # Define the location in MinIO
        bucket_name = "geospatial-results"
        object_name = f"{job_id}.geojson"
        
        # Ensure the bucket exists
        if not minio_client.bucket_exists(bucket_name):
            logger.info(f"[Job: {job_id}] Creating bucket: {bucket_name}")
            minio_client.make_bucket(bucket_name)
        
        # Upload the final GeoJSON file
        logger.info(f"[Job: {job_id}] Uploading result to MinIO: {bucket_name}/{object_name}")
        
        self.update_state(
            state='PROGRESS',
            meta={'stage': 'uploading_results', 'progress': 90, 'job_id': job_id}
        )
        
        minio_client.put_object(
            bucket_name=bucket_name,
            object_name=object_name,
            data=io.BytesIO(result_geojson_bytes),
            length=len(result_geojson_bytes),
            content_type="application/geo+json"
        )
        
        # Update Redis with the final success status
        if redis_client:
            redis_client.hset(
                f"job:{job_id}",
                mapping={
                    "status": "SUCCESS",
                    "progress": "100",
                    "bucket": bucket_name,
                    "object_name": object_name,
                    "features_count": str(len(result_gdf)),
                    "workflow_steps": str(len(plan.get('steps', []))),
                    "completed_at": str(time.time())
                }
            )
        
        logger.info(f"[Job: {job_id}] Successfully saved result to MinIO.")
        
        # Return the file's location for the API's status endpoint to use
        return {
            "bucket": bucket_name, 
            "object_name": object_name,
            "features_count": len(result_gdf)
        }
        
    except Exception as e:
        logger.error(f"[Job: {job_id}] Workflow execution FAILED. Error: {e}", exc_info=True)
        
        # Update Redis with the failure status
        if redis_client:
            redis_client.hset(
                f"job:{job_id}", 
                mapping={
                    "status": "FAILURE", 
                    "error": str(e)
                }
            )
        
        # Re-raise the exception for Celery to mark the task as FAILED
        raise

# You can keep this or other utility tasks if you need them.
@celery.task
def cleanup_old_jobs():
    """
    Cleanup task to remove old job data from Redis.
    This should be run periodically to prevent memory issues.
    """
    if not redis_client:
        return {"status": "skipped", "reason": "Redis not available"}
    
    try:
        # Get all job keys
        job_keys = redis_client.keys("job:*")
        cleaned_count = 0
        
        # Add real cleanup logic here later if needed
        # For now, just return the count of existing jobs
        
        return {
            "status": "completed",
            "total_jobs": len(job_keys),
            "cleaned_jobs": cleaned_count
        }
        
    except Exception as e:
        return {"status": "error", "error": str(e)}